# -*- coding: utf-8 -*-
"""StereoSet evaluation on BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p__i52qbgqCefCkcYm40uNGfAm_wdCMT
"""

# ! pip install datasets transformers tqdm

"""### BERT for Classification

We will start by considering a simple text-classification task. For this lab, we will use the RTE (Recognizing Textual Entailment) task. It is a two-input, two class classification task. 

Here are two examples:

```
Example 1:
  Premise: Two dogs are playing in the park.
  Hypothesis: There are animals in the park.
  Label: Entailment
```

```
Example 2:
  Premise: Two dogs are playing in the park.
  Hypothesis: There are cats in the park.
  Label: Not Entailment
```

Given the premise, we want to know if the premise **entails** the hypothesis.
"""

import numpy as np
import datasets
import torch
import torch.nn as nn
from transformers import AdamW
import transformers
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader
import tqdm.notebook as tqdm

# DEVICE = torch.device("cuda:0")
DEVICE = torch.device("cpu")
def to_device(batch):
  return {
      k: v.to(DEVICE)
      for k, v in batch.items()
  }

"""#### Data

First, let's go over how to use `datasets` to load the RTE task data.

The `datasets` library allows for very quick and easy access to many commonly used NLP datasets. It also has a very active open-source community, so new datasets are regularly added.

In the background, the `datasets` library will download the data to a cached location (usually somewhere in `~/.cache`).
"""

dataset = datasets.load_dataset('stereoset', 'intersentence')

# dataset

"""0 is anti-stereotype, while 1 is stereotype and 2 is unrelated"""

# dataset["validation"][0]

# id2Label = ["anti-stereotype", "stereotype", "unrelated"]

"""#### Preprocessing

Next, we will start to preprocess our dataset for using with RoBERTa.

As before, we first need to tokenize our dataset. We will using the tokenizer for `transformers`, and use the `from_pretrained` method to load the predefined vocabulary from the pretrained RoBERTa models.

The tokenizers in `transformers` handle many tokenization-related functionalities, including:

- Converting from strings to token IDs
- Converting back from token IDs to strings
- Doing additional preprocessing, such as adding additional `[CLS]` or padding tokens.
- Processing in parallel
"""

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

maxLen = 0
for ex in dataset["validation"]:
  for sentence in ex["sentences"]["sentence"]:
    if maxLen < len(sentence):
      maxLen = len(sentence)
      # print(maxLen, sentence)
      # break
  # break

# print(maxLen)

example = dataset["validation"][0]
tokenized_example = tokenizer(
    example["context"],
    example["sentences"]["sentence"][0], 
    max_length=170,
    padding="max_length",
    truncation=True,
)
#print(tokenized_example.keys())
#print(tokenized_example["input_ids"])
#print(tokenized_example["attention_mask"])
#print()
#print(tokenizer.decode(tokenized_example["input_ids"]))

"""Now, we can use the `.map` method in `datasets` to apply our tokenization over the whole dataset at once."""

def tokenize_example(example):
  tokenized_example = tokenizer(
      example["context"],
      example["sentences"]["sentence"][0], 
      max_length=170,
      padding="max_length",
      truncation=True,
  )
  tokenized_example["label"] = example["sentences"]["gold_label"][0]
  return tokenized_example

tokenized_dataset = dataset.map(
    tokenize_example,
    remove_columns=["context", "sentences", "id", "bias_type", "target"]
)

#print(tokenized_dataset["validation"][0])

"""From the tokenized dataset, we can built our usual data loaders.

`transformers` also comes with a pre-written `collate_fn` that works natively with dataset objects.
"""

BATCH_SIZE = 32
val_dataloader = DataLoader(
    tokenized_dataset["validation"],
    batch_size=BATCH_SIZE * 2,
    shuffle=False,
    collate_fn=transformers.data.DataCollatorWithPadding(tokenizer),
)

"""#### Model

Now, let's start building our model.

First, let's load the RoBERTa encoder. This gets us all the layers of the encoder except the MLM head, which we will not be using.
"""

encoder = transformers.BertForNextSentencePrediction.from_pretrained("bert-base-uncased")
encoder = encoder.to(DEVICE)

"""Let's take a look at what the encoder outputs."""

predictions = []

# for batch_num, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):
for batch in tqdm.tqdm(val_dataloader, desc="val", disable=True):
    input_ids=batch["input_ids"]
    token_type_ids=batch["token_type_ids"]
    attention_mask=batch["attention_mask"]
    # sentence_id=batch["labels"]
    # print(batch['input_ids'], input_ids)
    # break
    input_ids = input_ids.to(DEVICE)
    token_type_ids = token_type_ids.to(DEVICE)
    attention_mask = attention_mask.to(DEVICE)
    outputs = encoder(
        input_ids=batch["input_ids"],
        token_type_ids=batch["token_type_ids"],
        attention_mask=batch["attention_mask"],
        # labels=batch["labels"]
        )
    if type(outputs) == tuple:
        outputs = outputs[0]
    outputs = torch.softmax(outputs.logits, dim=1)

    for idx in range(input_ids.shape[0]):
        probabilities = {}
        # probabilities['id'] = sentence_id[idx]
        probabilities['score'] = outputs[idx, 0].item()
        predictions.append(probabilities)

with open('pred.txt', 'w') as f:
    for item in predictions:
        f.write("%s\n" % item)
